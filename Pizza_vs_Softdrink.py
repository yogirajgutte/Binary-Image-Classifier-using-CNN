# -*- coding: utf-8 -*-
"""Pizza vs Softdrink.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MVe0LcgUMxqc3ai5h1MuHd3oEo44JdAT
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/My Drive/Machine Learning/food_classifer_dataset.zip" -d "/content/"

# this is going to help in data processing
from keras.preprocessing.image import ImageDataGenerator

#these libraries are going to help in the model building.
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.layers import Activation, Dropout, Flatten, Dense

# dimensions of our images.
img_width, img_height = 150, 150

#setting up the directories
train_data_dir = '/content/classifer_dataset/train'
validation_data_dir = '/content/classifer_dataset/validate'

#setting up the batchsizes.
nb_train_samples = 8400
nb_validation_samples = 1600
#below are the hyerparameters
epochs = 50
batch_size = 16

input_shape = (img_width, img_height, 3)

# this is the augmentation configuration we will use for training
# augumentation generates more training images by rescaling, shearing, etc
train_datagen = ImageDataGenerator(
    rescale=1./ 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

#this generates batches of augment data for training
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

# this is the augmentation configuration we will use for validating
val_datagen = ImageDataGenerator(rescale=1./255)

#this generates batches of augment data for validating
validation_generator = val_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
#for extracting more features, I am adding more number of convolutional layers.
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

#flatten converts the image matrix to a 1-D vector.
model.add(Flatten())

model.add(Dense(512, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
#below dense function manages the output neuron.
model.add(Dense(1, activation='sigmoid'))

#configuring the model
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

#to print a summary representation of your model
model.summary()

#model training
model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size)

#to save the weights in the model as a HDFS file

model.save_weights('model_weight.h5')

#to save the architecture of the model as a json file

with open('model_architecture.json','w') as f:
    f.write(model.to_json())

from keras.models import load_model
from keras.models import model_from_json

# Model reconstruction from JSON file
with open('/content/food_model_architecture.json', 'r') as f:
    model = model_from_json(f.read())

# Load weights into the new model
model.load_weights('/content/first_try.h5')

import numpy as np
from keras.preprocessing import image
test_image=image.load_img('/content/images (1).jfif',target_size=(img_width,img_height))
test_image=image.img_to_array(test_image)
test_image=np.expand_dims(test_image,axis=0)
result=model.predict(test_image)
print(result)

if result[0][0]==1.0:
    prediction='SoftDrink'
else:
    prediction='Pizza'
print("You got a "+ prediction + " Yupeeeeeeeeeeeeeeeeeeeeeeeeeeeeee!!!")